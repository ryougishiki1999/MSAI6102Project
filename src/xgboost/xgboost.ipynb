{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stores-Item demand forecasting by XGBoost\n",
    "\n",
    "test lag, rolling features. v1.1\n",
    "\n",
    "## 1. Preparations\n",
    "\n",
    "Import related libraries, \n",
    "Set up global constants and Hyper parameters for XGBoost,\n",
    "Load train and test datasets.\n",
    "\n",
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedGroupKFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set up global constants and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./kaggle/input/demand-forecasting-kernels-only/\"\n",
    "TRAIN_FILENAME = \"train.csv\"\n",
    "TEST_FILENAME = \"test.csv\"\n",
    "TRAIN_FILEPATH = DATASET_PATH + TRAIN_FILENAME\n",
    "TEST_FILEPATH = DATASET_PATH + TEST_FILENAME\n",
    "\n",
    "# Attributes related to training/test data\n",
    "LABEL = \"sales\"\n",
    "COLUMN_DATE = \"date\"\n",
    "COLUMN_ID = \"id\"\n",
    "# orignial features\n",
    "FEATURE_STORE = \"store\"\n",
    "FEATURE_ITEM = \"item\"\n",
    "# created features\n",
    "FEATURE_DATE_YEAR = \"year\"\n",
    "FEATURE_DATE_MONTH = \"month\"\n",
    "FEATURE_DATE_QUARTER = \"quarter\"\n",
    "FEATURE_DATE_DAYOFWEEk = \"dayofweek\"\n",
    "FEATURE_DATE_DAYOFYEAR = \"dayofyear\"\n",
    "FEATURE_DATE_IS_MONTH_START = \"is_month_start\"\n",
    "FEATURE_DATE_IS_MONTH_END = \"is_month_end\"\n",
    "\n",
    "# For lag features\n",
    "LAGS = [\n",
    "    7, # 1 week\n",
    "    30, # 1 month\n",
    "    90, # 1 quarter, (max-min) of test dates\n",
    "    180, # a half year\n",
    "    365, # a complete year\n",
    "]\n",
    "\n",
    "# For Rolling Features\n",
    "MIN_PERIODS = 15 # half of month due to win_type\n",
    "WINDOWS = [\n",
    "    90, # 1 quarter, (max-min) of test dates + MIN_PERIODS\n",
    "    180, # a half year + MIN_PERIODS\n",
    "    365, # a complete year + MIN_PERIODS\n",
    "]\n",
    "WIN_TYPE = \"triang\"\n",
    "\n",
    "\n",
    "# initial featuren name collection\n",
    "all_feature_names = [\n",
    "    FEATURE_STORE,\n",
    "    FEATURE_ITEM,\n",
    "    FEATURE_DATE_YEAR,\n",
    "    FEATURE_DATE_MONTH,\n",
    "    FEATURE_DATE_QUARTER,\n",
    "    FEATURE_DATE_DAYOFWEEk,\n",
    "    FEATURE_DATE_DAYOFYEAR,\n",
    "    FEATURE_DATE_IS_MONTH_START,\n",
    "    FEATURE_DATE_IS_MONTH_END,\n",
    "]\n",
    "\n",
    "\n",
    "NDIGITS = 4 # precison of round() method\n",
    "N_SPLITS = 5 # for Kfold\n",
    "\n",
    "COLOR_PAL = [\"#F8766D\", \"#D39200\", \"#93AA00\", \"#00BA38\", \"#00C19F\", \"#00B9E3\", \"#619CFF\", \"#DB72FB\"]\n",
    "plt.style.use(style=\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Set up Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## General Parameters\n",
    "\n",
    "BOOSTER = \"gbtree\" # gbtree and dart use tree based; gblinear use linear funcitons\n",
    "DEVICE = \"cuda\" # cuda, gpu or cpu\n",
    "VALIDATE_PARAMETERS = True # perform validation of input parameters to check whether a parameter is used or not.\n",
    "DISABLE_DEFEAULT_EVAL_METRIC = True # use customized sample instead.\n",
    "\n",
    "## Parameters for Tree Booster\n",
    "\n",
    "ETA = 0.3 # default 0,3, learning_rate\n",
    "GAMMA = 0 # default 0, min_split_loss, The larger the more conservative algorithm is.\n",
    "MAX_DEPTH = 6 # default 6, 0 indicates no limit on depth.\n",
    "# defalut 1, In regression task, simply corresponds to minimum number of instances in each node.\n",
    "# The larger hte more conservative\n",
    "MIN_CHILD_WEIGHT = 1 \n",
    "# # default 0, positive to help making the update step more conservative.\n",
    "# #it might help in logistic regression when class is extremely imbalanced.\n",
    "# MAX_DELTA_STEP = 0 \n",
    "SUBSAMPLE = 0.5 # default 1, range(0,1], this prevent overfitting\n",
    "COLSAMPLE_BYTREE = 0.8 # range(0,1]\n",
    "# LAMBDA = 1 #default 1, alias: reg_lambda, L2 regularization term on weights increase to make more conservative\n",
    "# ALPHA = 0 #defalut 0, L1 regularization term on weights\n",
    "TREE_METHOD = \"hist\" # tree method of xgboost: auto=hist, exact, approx, hist\n",
    "\n",
    "## Learning Task Parameters\n",
    "\n",
    "OBJECTIVE = \"reg:squarederror\" # objective of learning task\n",
    "num_boost_round = 150 # default 10, equivalent to number of gradient boosted trees.\n",
    "early_stopping_rounds = (lambda x: x//10)\n",
    "RANDOM_STATE = 214\n",
    "\n",
    "## Booster parameters of XGBoost model\n",
    "\n",
    "xgb_params = {\n",
    "    'booster': BOOSTER,\n",
    "    'device': DEVICE,\n",
    "    'validate_parameters': VALIDATE_PARAMETERS,\n",
    "    'disable_default_eval_metric': DISABLE_DEFEAULT_EVAL_METRIC,\n",
    "    'objective': OBJECTIVE,\n",
    "    'min_child_weight': MIN_CHILD_WEIGHT,\n",
    "    # 'max_delta_step': MAX_DELTA_STEP,\n",
    "    'subsample': SUBSAMPLE,\n",
    "    'colsample_bytree': COLSAMPLE_BYTREE,\n",
    "    # 'lambda': LAMBDA,\n",
    "    # 'alpha': ALPHA,\n",
    "    'tree_method': TREE_METHOD,\n",
    "    # 'n_estimators': NUM_BOOST_ROUND,\n",
    "}\n",
    "\n",
    "\n",
    "## Parameters for XGBoost cross validation: xgb.cv()\n",
    "\n",
    "NFOLD = 5\n",
    "STRATIFIED = True # perform stratified sampling\n",
    "SHUFFLE = False\n",
    "AS_PANDAS = True # Return pd.DataFrame\n",
    "\n",
    "# Parmaeters for BayesianSearchCV\n",
    "\n",
    "CV = 3\n",
    "SCORING = None #'neg_mean_squared_error'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_FILEPATH, parse_dates=[COLUMN_DATE], index_col=COLUMN_DATE)\n",
    "test_df = pd.read_csv(TEST_FILEPATH, parse_dates=[COLUMN_DATE], index_col=COLUMN_DATE)\n",
    "raw_data_concact_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Time Features, \n",
    "2. Lag Features, \n",
    "3. Rolling Features\n",
    "4. One-Hot coding for categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(raw_df: pd.DataFrame, label:str = None):\n",
    "    def create_time_features(data_df: pd.DataFrame):\n",
    "        data_df[FEATURE_DATE_YEAR] = data_df.index.year\n",
    "        data_df[FEATURE_DATE_MONTH] = data_df.index.month\n",
    "        data_df[FEATURE_DATE_QUARTER] = data_df.index.quarter\n",
    "        data_df[FEATURE_DATE_DAYOFYEAR] = data_df.index.dayofyear\n",
    "        data_df[FEATURE_DATE_DAYOFWEEk] = data_df.index.dayofweek\n",
    "        data_df[FEATURE_DATE_IS_MONTH_START] = (data_df.index.is_month_start).astype(int)\n",
    "        data_df[FEATURE_DATE_IS_MONTH_END] = (data_df.index.is_month_end).astype(int)\n",
    "        \n",
    "    def create_lag_features(data_df: pd.DataFrame):\n",
    "        for lag in LAGS:\n",
    "            feature_name = \"sales_lag_\" + str(lag)\n",
    "            all_feature_names.append(feature_name)\n",
    "            data_df[feature_name] = \\\n",
    "                data_df.groupby(by=[FEATURE_STORE, FEATURE_ITEM])[LABEL].\\\n",
    "                    shift(periods=lag)\n",
    "                \n",
    "    def create_rolling_mean_features(data_df: pd.DataFrame):\n",
    "        for window in WINDOWS:\n",
    "            feature_mean_name = \"sales_rolling_mean_\" + str(window)\n",
    "            feature_median_name = \"sale_rolling_median_\" + str(window)\n",
    "            all_feature_names.append(feature_mean_name)\n",
    "            all_feature_names.append(feature_median_name)\n",
    "            \n",
    "            data_df[feature_mean_name] = \\\n",
    "                data_df.groupby(by=[FEATURE_STORE, FEATURE_ITEM])[LABEL].\\\n",
    "                    transform(lambda x: x.rolling(window=window, min_periods = MIN_PERIODS, win_type = \"triang\").mean())\n",
    "            data_df[feature_median_name] = \\\n",
    "                data_df.groupby(by=[FEATURE_STORE, FEATURE_ITEM])[LABEL].\\\n",
    "                    transform(lambda x: x.rolling(window=window, min_periods = MIN_PERIODS).median())\n",
    "    \n",
    "    def create_one_hot_encoding(data_df: pd.DataFrame,\n",
    "                                one_hot_encoding_cols = [\n",
    "                                    FEATURE_STORE\n",
    "                                ]):\n",
    "        # TODO, not finised yet.\n",
    "        data_df  = pd.get_dummies(data_df, columns=one_hot_encoding_cols)\n",
    "        return data_df\n",
    "    \n",
    "    data_df = copy.deepcopy(raw_df)\n",
    "    create_time_features(data_df)\n",
    "    create_lag_features(data_df)\n",
    "    create_rolling_mean_features(data_df)\n",
    "    # data_df = create_one_hot_encoding(data_df)\n",
    "\n",
    "    X_data = data_df.loc[:, all_feature_names]\n",
    "    \n",
    "    if label:\n",
    "        y_data = raw_df[label]\n",
    "        return data_df, X_data, y_data\n",
    "    else:\n",
    "        return data_df, X_data\n",
    "    \n",
    "data_df, X_data, y_data = create_features(raw_data_concact_df, label=LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define customized metrics : smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(predt: np.ndarray, dtrain: xgb.DMatrix)->tuple[str, float]:\n",
    "    y = dtrain.get_label()\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    elements = np.abs(predt-y)/((np.abs(predt)+np.abs(y))*0.5)\n",
    "    return 'smape', round(np.mean(elements)*100, ndigits=NDIGITS)\n",
    "\n",
    "def smape_np(predt: np.ndarray, actual:np.ndarray)->float:\n",
    "    elements = np.abs(predt-actual)/((np.abs(predt)+np.abs(actual))*0.5)\n",
    "    return round(np.mean(elements)*100, ndigits=NDIGITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.sort_index(inplace=True)\n",
    "\n",
    "train_df = data_df.loc[~data_df.sales.isna()]\n",
    "X_train = train_df[all_feature_names]\n",
    "y_train = train_df[LABEL]\n",
    "\n",
    "test_df = data_df[data_df.sales.isna()]\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df.set_index(keys = [COLUMN_ID], inplace=True)\n",
    "test_df = test_df.sort_index()\n",
    "X_test = test_df[all_feature_names]\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost cross validation\n",
    "apply initial `xgb_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_historys = xgb.cv(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    early_stopping_rounds=early_stopping_rounds(num_boost_round),\n",
    "    nfold=NFOLD,\n",
    "    seed=RANDOM_STATE,\n",
    "    stratified=STRATIFIED,\n",
    "    as_pandas=AS_PANDAS,\n",
    "    shuffle=SHUFFLE,\n",
    "    custom_metric=smape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_historys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = eval_historys.shape[0]\n",
    "xgb_regressor = \\\n",
    "    xgb.XGBRegressor(**xgb_params,\n",
    "                    n_estimators = num_boost_round,\n",
    "                    eval_metric = smape,\n",
    "                    random_state = RANDOM_STATE\n",
    "                    )\n",
    "\n",
    "xgb_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb_regressor.predict(X_train)\n",
    "print(\"SMAPE score:\",smape_np(y_train,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "tune_params_grid = {\n",
    "    'max_depth': np.arange(3,12,1),\n",
    "    'min_child_weight': np.arange(0.05, 1.0, 0.05),\n",
    "    'gamma': np.arange(0.0,5,0.05),\n",
    "    'learning_rate': np.arange(0.05,0.3,0.005),\n",
    "    'subsample': np.arange(0.5,1.0,0.05),\n",
    "    'colsample_bytree': np.arange(0.5,1.0,0.05),\n",
    "}\n",
    "\n",
    "tuned_xgb = \\\n",
    "    BayesSearchCV(\n",
    "        xgb_regressor,\n",
    "        tune_params_grid,\n",
    "        cv=CV,\n",
    "        scoring=SCORING,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "tuned_xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"best_params = {tuned_xgb.best_params_}\\n\" +\n",
    "    f\"bet_scores = {tuned_xgb.best_score_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = tuned_xgb.best_estimator_.get_booster()\n",
    "xgb.plot_importance(\n",
    "    booster=bst\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(data=X_test)\n",
    "predictions = bst.predict(dtest)\n",
    "test_df[LABEL] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df.loc[:,[LABEL]]\n",
    "submission_df.index = submission_df.index.astype(int)\n",
    "submission_df.to_csv(\"submission.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML6102project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
