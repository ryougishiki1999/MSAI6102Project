{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"454d802d65270b418e4d05e9cfd3093edba6cd25","trusted":true},"outputs":[],"source":["import os\n","import gc\n","import time\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import lightgbm as lgb"]},{"cell_type":"markdown","metadata":{"_uuid":"d519f87e986a4caff405792bdf5f88af064dd4b8"},"source":["## Loading data"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"40c4ad4e03ff3701bacf112e0cfb79321f0456e5","trusted":true},"outputs":[],"source":["# Loading the data\n","train = pd.read_csv('./input/train.csv', parse_dates=['date'])\n","test = pd.read_csv('./input/test.csv', parse_dates=['date'])\n","sample_sub = pd.read_csv('./input/sample_submission.csv')\n","print('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))\n","train.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"03f82077e80bba187f3ff0d0cf58ba973c13c7ef"},"source":["## Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"49f8496f37589f16e61d1f2aa965412a14ec7069","trusted":true},"outputs":[],"source":["# Concatenating train & test\n","train['train_or_test'] = 'train'\n","test['train_or_test'] = 'test'\n","df = pd.concat([train,test], sort=False)\n","print('Combined df shape:{}'.format(df.shape))\n","del train, test\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"_uuid":"c2c63fcf7e65e09b015f0d5aced7a2e0e009daf2"},"source":["### Date Features"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"424bf991b29ea359b007bcd8c2a8cdf054c1cc1f","trusted":true},"outputs":[],"source":["# Extracting date features\n","df['dayofmonth'] = df.date.dt.day\n","df['dayofyear'] = df.date.dt.dayofyear\n","df['dayofweek'] = df.date.dt.dayofweek\n","df['month'] = df.date.dt.month\n","df['year'] = df.date.dt.year\n","#df['weekofyear'] = df.date.dt.weekofyear\n","df['is_month_start'] = (df.date.dt.is_month_start).astype(int)\n","df['is_month_end'] = (df.date.dt.is_month_end).astype(int)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"328bae42ea8aa58ebc512be340c55d3f05918b94","trusted":true},"outputs":[],"source":["# Sorting the dataframe by store then item then date\n","#df.sort_values(by=['store','item','month','dayofweek'], axis=0, inplace=True)\n","df.sort_values(by=['store','item','date'], axis=0, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"_uuid":"019db2cf8e367672bb2339330767de1b70fae4cd"},"source":["### Monthwise aggregated sales values"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"28475aa2a93911d3149c2c38f369b22c12ab58b3","trusted":true},"outputs":[],"source":["def create_sales_agg_monthwise_features(df, gpby_cols, target_col, agg_funcs):\n","    '''\n","    Creates various sales agg features with given agg functions  \n","    '''\n","    gpby = df.groupby(gpby_cols)\n","    newdf = df[gpby_cols].drop_duplicates().reset_index(drop=True)\n","    for agg_name, agg_func in agg_funcs.items():\n","        aggdf = gpby[target_col].agg(agg_func).reset_index()\n","        aggdf.rename(columns={target_col:target_col+'_'+agg_name}, inplace=True)\n","        newdf = newdf.merge(aggdf, on=gpby_cols, how='left')\n","    return newdf"]},{"cell_type":"markdown","metadata":{"_uuid":"f3caa2627c7c33bc8c273daf3ba87fada453a5ca"},"source":["### Features constructed from previous sales values"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"79559cff98ede8c16cff2f7f7d575436112488ae","trusted":true},"outputs":[],"source":["# Creating sales lag features\n","def create_sales_lag_feats(df, gpby_cols, target_col, lags):\n","    gpby = df.groupby(gpby_cols)\n","    for i in lags:\n","        df['_'.join([target_col, 'lag', str(i)])] = \\\n","                gpby[target_col].shift(i).values + np.random.normal(scale=1.6, size=(len(df),))\n","        print(gpby[target_col].shift(i).values)\n","    return df\n","\n","# Creating sales rolling mean features\n","def create_sales_rmean_feats(df, gpby_cols, target_col, windows, min_periods=2, \n","                             shift=1, win_type=None):\n","    gpby = df.groupby(gpby_cols)\n","    for w in windows:\n","        df['_'.join([target_col, 'rmean', str(w)])] = \\\n","            gpby[target_col].shift(shift).rolling(window=w, \n","                                                  min_periods=min_periods,\n","                                                  win_type=win_type).mean().values +\\\n","            np.random.normal(scale=1.6, size=(len(df),))\n","    return df\n","\n","# Creating sales rolling median features\n","def create_sales_rmed_feats(df, gpby_cols, target_col, windows, min_periods=2, \n","                            shift=1, win_type=None):\n","    gpby = df.groupby(gpby_cols)\n","    for w in windows:\n","        df['_'.join([target_col, 'rmed', str(w)])] = \\\n","            gpby[target_col].shift(shift).rolling(window=w, \n","                                                  min_periods=min_periods,\n","                                                  win_type=win_type).median().values +\\\n","            np.random.normal(scale=1.6, size=(len(df),))\n","    return df\n","\n","# Creating sales exponentially weighted mean features\n","def create_sales_ewm_feats(df, gpby_cols, target_col, alpha=[0.9], shift=[1]):\n","    gpby = df.groupby(gpby_cols)\n","    for a in alpha:\n","        for s in shift:\n","            df['_'.join([target_col, 'lag', str(s), 'ewm', str(a)])] = \\\n","                gpby[target_col].shift(s).ewm(alpha=a).mean().values\n","    return df"]},{"cell_type":"markdown","metadata":{"_uuid":"1abfbefd0f70daebfdce943d4a292d8a8d7f71be"},"source":["### OHE of categorical features"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ee278af552f8180d1d087979104995d89f5736ff","trusted":true},"outputs":[],"source":["def one_hot_encoder(df, ohe_cols=['store','item','dayofmonth','dayofweek','month','weekofyear']):\n","    '''\n","    One-Hot Encoder function\n","    '''\n","    print('Creating OHE features..\\nOld df shape:{}'.format(df.shape))\n","    df = pd.get_dummies(df, columns=ohe_cols)\n","    print('New df shape:{}'.format(df.shape))\n","    return df"]},{"cell_type":"markdown","metadata":{"_uuid":"69d6af2b8e69c797cc27b887c0e87050ec18870d"},"source":["### Log Sales "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9075af279047a4b90ef6d2bb96c268058621452b","trusted":true},"outputs":[],"source":["# Converting sales to log(1+sales)\n","df['sales'] = np.log1p(df.sales.values)\n","df.sample(2)"]},{"cell_type":"markdown","metadata":{"_uuid":"06b09956138bd3ef34009606aba6cf6f5065ae02"},"source":["## Time-based Validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e4d32958c67c166e1fe59b73bbe5b674c7335c52","trusted":true},"outputs":[],"source":["# For validation \n","# We can choose last 3 months of training period(Oct, Nov, Dec 2017) as our validation set to gauge the performance of the model.\n","# OR to keep months also identical to test set we can choose period (Jan, Feb, Mar 2017) as the validation set.\n","# Here we will go with the latter choice.\n","masked_series = (df.year==2017) & (df.month.isin([1,2,3]))\n","masked_series2 = (df.year==2017) & (~(df.month.isin([1,2,3])))\n","df.loc[(masked_series), 'train_or_test'] = 'val'\n","df.loc[(masked_series2), 'train_or_test'] = 'no_train'\n","print('Train shape: {}'.format(df.loc[df.train_or_test=='train',:].shape))\n","print('Validation shape: {}'.format(df.loc[df.train_or_test=='val',:].shape))\n","print('No train shape: {}'.format(df.loc[df.train_or_test=='no_train',:].shape))\n","print('Test shape: {}'.format(df.loc[df.train_or_test=='test',:].shape))"]},{"cell_type":"markdown","metadata":{"_uuid":"69c7c13414eb865e98745489ab36bfb3d0b98a07"},"source":["## Model Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3e5de9c3d49c9a95887a8e7e9d110e2e72851594","trusted":true},"outputs":[],"source":["# Converting sales of validation period to nan so as to resemble test period\n","train = df.loc[df.train_or_test.isin(['train','val']), :]\n","Y_val = train.loc[train.train_or_test=='val', 'sales'].values.reshape((-1))\n","Y_train = train.loc[train.train_or_test=='train', 'sales'].values.reshape((-1))\n","train.loc[train.train_or_test=='val', 'sales'] = np.nan\n","\n","# # Creating sales lag, rolling mean, rolling median, ohe features of the above train set\n","train = create_sales_lag_feats(train, gpby_cols=['store','item'], target_col='sales', \n","                               lags=[91,98,105,112,119,126,182,364,546,728])\n","\n","train = create_sales_rmean_feats(train, gpby_cols=['store','item'], \n","                                 target_col='sales', windows=[364,546], \n","                                 min_periods=10, win_type='triang') #98,119,91,182,\n","\n","# # train = create_sales_rmed_feats(train, gpby_cols=['store','item'], \n","# #                                 target_col='sales', windows=[364,546], \n","# #                                 min_periods=10, win_type=None) #98,119,91,182,\n","\n","train = create_sales_ewm_feats(train, gpby_cols=['store','item'], \n","                               target_col='sales', \n","                               alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5], \n","                               shift=[91,98,105,112,119,126,182,364,546,728])\n","\n","# # Creating sales monthwise aggregated values\n","# agg_df = create_sales_agg_monthwise_features(df.loc[df.train_or_test=='train', :], \n","#                                              gpby_cols=['store','item','month'], \n","#                                              target_col='sales', \n","#                                              agg_funcs={'mean':np.mean, \n","#                                              'median':np.median, 'max':np.max, \n","#                                              'min':np.min, 'std':np.std})\n","\n","# # Joining agg_df with train\n","# train = train.merge(agg_df, on=['store','item','month'], how='left')\n","\n","# One-Hot Encoding \n","train = one_hot_encoder(train, ohe_cols=['store','item','dayofweek','month']) \n","#,'dayofmonth','weekofyear'\n","\n","# Final train and val datasets\n","val = train.loc[train.train_or_test=='val', :]\n","train = train.loc[train.train_or_test=='train', :]\n","print('Train shape:{}, Val shape:{}'.format(train.shape, val.shape))"]},{"cell_type":"markdown","metadata":{"_uuid":"a7732144f23f0a99b5846ca8315455b84f1ca46e"},"source":["## LightGBM Model"]},{"cell_type":"markdown","metadata":{"_uuid":"17982e472879d213fd8e771c3156c89cdd954388"},"source":["### Training features"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"_uuid":"3c33f479d52628e200d2905e7c78ba43238be2a7","trusted":true},"outputs":[],"source":["avoid_cols = ['date', 'sales', 'train_or_test', 'id', 'year']\n","cols = [col for col in train.columns if col not in avoid_cols]\n","print('No of training features: {} \\nAnd they are:{}'.format(len(cols), cols))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"de085b7a91c27ff28248735e6c1387f432131644","trusted":true},"outputs":[],"source":["def smape(preds, target):\n","    '''\n","    Function to calculate SMAPE\n","    '''\n","    n = len(preds)\n","    masked_arr = ~((preds==0)&(target==0))\n","    preds, target = preds[masked_arr], target[masked_arr]\n","    num = np.abs(preds-target)\n","    denom = np.abs(preds)+np.abs(target)\n","    smape_val = (200*np.sum(num/denom))/n\n","    return smape_val\n","\n","def lgbm_smape(preds, train_data):\n","    '''\n","    Custom Evaluation Function for LGBM\n","    '''\n","    labels = train_data.get_label()\n","    smape_val = smape(np.expm1(preds), np.expm1(labels))\n","    return 'SMAPE', smape_val, False"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bebd7beef960cb6bcdf39a66ce1f5bb09ec23e04","trusted":true},"outputs":[],"source":["# LightGBM parameters\n","lgb_params = {'task':'train', 'boosting_type':'gbdt', 'objective':'regression', \n","              'metric': {'mae'}, 'num_leaves': 10, 'learning_rate': 0.02, \n","              'feature_fraction': 0.8, 'max_depth': 5, 'verbose': 0, \n","              'num_boost_round':15000, 'early_stopping_rounds':200, 'nthread':-1}"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0f6c0b39b78177a733641da24c42abe18c2a33d9","trusted":true},"outputs":[],"source":["# Creating lgbtrain & lgbval\n","lgbtrain = lgb.Dataset(data=train.loc[:,cols].values, label=Y_train, \n","                       feature_name=cols)\n","lgbval = lgb.Dataset(data=val.loc[:,cols].values, label=Y_val, \n","                     reference=lgbtrain, feature_name=cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2bf8adfe7d434402acaaa0b03399937f24dcd007","trusted":true},"outputs":[],"source":["def lgb_validation(params, lgbtrain, lgbval, X_val, Y_val, verbose_eval):\n","    t0 = time.time()\n","    evals_result = {}\n","    model = lgb.train(params, lgbtrain, num_boost_round=params['num_boost_round'], \n","                      valid_sets=[lgbtrain, lgbval], feval=lgbm_smape, \n","                      early_stopping_rounds=params['early_stopping_rounds'], \n","                      evals_result=evals_result, verbose_eval=verbose_eval)\n","    print(model.best_iteration)\n","    print('Total time taken to build the model: ', (time.time()-t0)/60, 'minutes!!')\n","    pred_Y_val = model.predict(X_val, num_iteration=model.best_iteration)\n","    pred_Y_val = np.expm1(pred_Y_val)\n","    Y_val = np.expm1(Y_val)\n","    val_df = pd.DataFrame(columns=['true_Y_val','pred_Y_val'])\n","    val_df['pred_Y_val'] = pred_Y_val\n","    val_df['true_Y_val'] = Y_val\n","    print(val_df.shape)\n","    print(val_df.sample(5))\n","    print('SMAPE for validation data is:{}'.format(smape(pred_Y_val, Y_val)))\n","    return model, val_df"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"21eb1034ea7d6b6523d21aceb98849c9a1f29ce9","trusted":true},"outputs":[],"source":["# Training lightgbm model and validating\n","model, val_df = lgb_validation(lgb_params, lgbtrain, lgbval, val.loc[:,cols].values, \n","                               Y_val, verbose_eval=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d9f60a5bce135813adf2ad8524ff72a67f72fb7b","trusted":true},"outputs":[],"source":["# Let's see top 25 features as identified by the lightgbm model.\n","print(\"Features importance...\")\n","gain = model.feature_importance('gain')\n","feat_imp = pd.DataFrame({'feature':model.feature_name(), \n","                         'split':model.feature_importance('split'), \n","                         'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n","print('Top 25 features:\\n', feat_imp.head(25))"]},{"cell_type":"markdown","metadata":{"_uuid":"70a1432509985f18bdb6dd1535838db297a22658"},"source":["## Final Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a8781fd8f0fb9e98559a2c4f99519fc91c633e50","trusted":true},"outputs":[],"source":["# Creating sales lag, rolling mean, rolling median, ohe features of the above train set\n","df_whole = create_sales_lag_feats(df, gpby_cols=['store','item'], target_col='sales', \n","                                  lags=[91,98,105,112,119,126,182,364,546,728])\n","df_whole = create_sales_rmean_feats(df_whole, gpby_cols=['store','item'], \n","                                    target_col='sales', windows=[364,546], \n","                                    min_periods=10, win_type='triang')\n","# df = create_sales_rmed_feats(df, gpby_cols=['store','item'], target_col='sales', \n","#                              windows=[364,546], min_periods=2) #98,119,\n","df_whole = create_sales_ewm_feats(df_whole, gpby_cols=['store','item'], target_col='sales', \n","                                  alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5], \n","                                  shift=[91,98,105,112,119,126,182,364,546,728])\n","\n","# # Creating sales monthwise aggregated values\n","# agg_df = create_sales_agg_monthwise_features(df.loc[~(df.train_or_test=='test'), :], \n","#                                              gpby_cols=['store','item','month'], \n","#                                              target_col='sales', \n","#                                              agg_funcs={'mean':np.mean, \n","#                                              'median':np.median, 'max':np.max, \n","#                                              'min':np.min, 'std':np.std})\n","\n","# # Joining agg_df with df\n","# df = df.merge(agg_df, on=['store','item','month'], how='left')\n","\n","# One-Hot Encoding\n","df_whole = one_hot_encoder(df_whole, ohe_cols=['store','item','dayofweek','month']) \n","#'dayofmonth',,'weekofyear'\n","\n","# Final train and test datasets\n","test = df_whole.loc[df_whole.train_or_test=='test', :]\n","train = df_whole.loc[~(df_whole.train_or_test=='test'), :]\n","print('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9d93caa77885762a3e6729341414f54c1c1d0675","trusted":true},"outputs":[],"source":["# LightGBM dataset\n","lgbtrain_all = lgb.Dataset(data=train.loc[:,cols].values, \n","                           label=train.loc[:,'sales'].values.reshape((-1,)), \n","                           feature_name=cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"051cf13823f8edf3810e88016c90db5c14054ef2","trusted":true},"outputs":[],"source":["def lgb_train(params, lgbtrain_all, X_test, num_round):\n","    t0 = time.time()\n","    model = lgb.train(params, lgbtrain_all, num_boost_round=num_round, feval=lgbm_smape)\n","    test_preds = model.predict(X_test, num_iteration=num_round)\n","    print('Total time taken in model training: ', (time.time()-t0)/60, 'minutes!')\n","    return model, test_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4192fdca974d3c4fade051fac2e87723f12983b7","trusted":true},"outputs":[],"source":["# Training lgb model on whole data(train+val)\n","lgb_model, test_preds = lgb_train(lgb_params, lgbtrain_all, test.loc[:,cols].values, model.best_iteration)\n","print('test_preds shape:{}'.format(test_preds.shape))"]},{"cell_type":"markdown","metadata":{"_uuid":"23056dbbe715566d1cba5d1b6dafc3d9d972d128"},"source":["## Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1aba1607a8f47480213035f7af77f9ac6f9c30c7","scrolled":true,"trusted":true},"outputs":[],"source":["# Create submission\n","sub = test.loc[:,['id','sales']]\n","sub['sales'] = np.expm1(test_preds)\n","sub['id'] = sub.id.astype(int)\n","sub.to_csv('submission.csv', index=False)\n","sub.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"35eb0e52bff6541e1f948f6d43dab6a105818d67"},"source":["## WaveNet Model "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bb5af7ea2e8f0cb18f6fc30061ede9422634262b","trusted":true},"outputs":[],"source":["df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8fe1b567de6709709f6310a760e284395510a5db","trusted":true},"outputs":[],"source":["df.date.min(), df.date.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ae6542095a3558301661f2ccb6317094f14c7e17","trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":1}
